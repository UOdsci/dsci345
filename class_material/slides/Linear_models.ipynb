{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202599bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to linear models\n",
    "\n",
    "Peter Ralph\n",
    "\n",
    "https://uodsci.github.io/dsci345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bfdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 8)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dsci345 import pretty\n",
    "import sklearn\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec5c59",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\\renewcommand{\\P}{\\mathbb{P}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\var}{\\text{var}} \\newcommand{\\sd}{\\text{sd}} \\newcommand{\\cov}{\\text{cov}} \\newcommand{\\cor}{\\text{cor}}$$\n",
    "This is here so we can use `\\P` and `\\E` and `\\var` and `\\cov` and `\\cor` and `\\sd` in LaTeX below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f60c3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A first linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b354d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we suppose two random variables $X$ and $Y$ depend on each other,\n",
    "perhaps the simplest set-up is that\n",
    "$$ Y = a X + b + \\epsilon, $$\n",
    "where\n",
    "- $a$ is the *slope*, i.e., how much $Y$ goes up by, on average, per unit of increase in $X$,\n",
    "- $b$ is the *intercept*, i.e., the expected value of $Y$ when $X=0$ (if this makes sense),\n",
    "- $\\hat Y = a X + b$ is the *predicted value* of $Y$ given $X$,\n",
    "- $\\epsilon = Y - \\hat Y$ is the *residual*, which for the above to be true must have mean zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6756de8",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c81ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose $X$ and $Y$ are two, related, measurements,\n",
    "which we treat as random variables having some joint distribution.\n",
    "*Question:* If we know the value of $X$, how are we to best$^*$ predict the value of $Y$?\n",
    "\n",
    "$^*$ where how about \"best\" means \"with smallest mean squared error\" (i.e., smallest MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6008c4a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So: we'd like to define an estimator of $Y$,\n",
    "which we'll call $\\hat Y$,\n",
    "and will be a function of $X$ (and only $X$),\n",
    "and we'd like that estimator to minimize\n",
    "$$ \\E[(Y - \\hat Y)^2] \\qquad \\text{(the MSE)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead5311",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First observation\n",
    "\n",
    "If we're allowed to add a constant to our estimator,\n",
    "then the mean of the optimal estimator, $\\hat Y$, must match that of $Y$:\n",
    "$$ \\E[\\hat Y] = \\E[Y] . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bbd639",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Note:* for a counterexample about the \"if we're allowed to add a constant\"\n",
    "note, suppose we're looking for an $a$ such that $\\hat Y = \\exp(a X)$;\n",
    "then $\\hat Y + m$ is *not* allowed, as it doesn't take the same form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbb2f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Proof:*\n",
    "Suppose that $\\tilde Y$ is an estimator;\n",
    "consider the MSE for $\\tilde Y + m$, where $m$ is a number,\n",
    "which we define to be $f(m)$:\n",
    "$$\\begin{aligned}\n",
    " f(m)\n",
    " &=\n",
    " \\E[(Y - (\\tilde Y + m))^2] \\\\\n",
    " &=\n",
    " \\E[(Y - \\tilde Y)^2]\n",
    " - 2 m \\E[Y - \\tilde Y]\n",
    " + m^2 .\n",
    "\\end{aligned}$$\n",
    "We want to find the value of $m$ that minimizes $f(m)$,\n",
    "so differentiate with respect to $m$:\n",
    "$$\\begin{aligned}\n",
    " f'(m)\n",
    " &=\n",
    " - 2\\E[Y - \\tilde Y]\n",
    " + 2 m .\n",
    "\\end{aligned}$$\n",
    "Setting this equal to zero, we find that $f'(m) = 0$ if\n",
    "$$ m = \\E[Y - \\tilde Y] = \\E[Y] - \\E[\\tilde Y]. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1d60e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In conclusion, we've found out that if $\\tilde Y$ is an estimator,\n",
    "then $\\hat Y = \\tilde Y + \\E[Y] - \\E[\\tilde Y]$ is a *better* estimator.\n",
    "And, notice that (by additivity of means),\n",
    "$\\E[\\hat Y] = \\E[Y]$.\n",
    "\n",
    "So, any estimator of $Y$ can be improved (in the mean-squared-error sense)\n",
    "by shifting it so that the mean of the estimator is equal to the mean of $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ec275",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Second observation\n",
    "\n",
    "If the estimator $\\hat Y$ is *linear*,\n",
    "i.e.,\n",
    "$$ \\hat Y = a X + b $$\n",
    "for some $a$ and $b$, then\n",
    "$$\n",
    "a = \\frac{\\sd[Y]}{\\sd[X]} \\cor[X, Y] ,\n",
    "$$\n",
    "and $b$ is chosen so that $\\E[\\hat Y] = \\E[Y]$:\n",
    "$$ b = \\E[Y] - a \\E[X] .$$\n",
    "\n",
    "In words, the slope, $a$,\n",
    "is equal to the correlation between $X$ and $Y$,\n",
    "but in units of standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853abf0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Proof:*\n",
    "We'll do just the same thing as above.\n",
    "First, thanks to the first observation,\n",
    "we can assume $\\E[X] = \\E[Y] = 0$,\n",
    "which implies that $b = 0$,\n",
    "and $\\E[X^2] = \\var[X]$ and $\\E[Y^2] = \\var[Y]$\n",
    "and $\\E[XY] = \\cov[X,Y]$.\n",
    "Now,\n",
    "$$\\begin{aligned}\n",
    "    \\text{(MSE)}\n",
    "    &= \\E[(Y - \\hat Y)^2 ] \\\\\n",
    "    &= \\E[(Y - a X)^2 ] \\\\\n",
    "    &= \\E[Y^2 - 2 a X Y + a^2 X^2 ] \\\\\n",
    "    &= \\var[Y] - 2 a \\cov[X, Y] + a^2 \\var[X] .\n",
    "\\end{aligned}$$\n",
    "If we differentiate this with respect to $a$\n",
    "and set it equal to zero,\n",
    "we find that the MSE is minimized at\n",
    "$$ a = \\frac{\\cov[X,Y]}{\\var[X]} . $$\n",
    "If we now substitute in for $\\cov[X,Y] = \\sd[X]\\sd[Y]\\cor[X,Y]$,\n",
    "we get the form of $a$ above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e1aaab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Normal distribution and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501da55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How good the estimation procedure above works\n",
    "depends on the situation, of course.\n",
    "Here's one concrete situation - i.e., model -\n",
    "that we might expect to (a) describe a lot of real-world situations pretty well,\n",
    "and (b) be well-suited to fitting with a linear model by least squares:\n",
    "$$\\begin{aligned}\n",
    "    Y &\\sim \\text{Normal}(\\text{mean}=a X + b, \\text{sd}=\\sigma) .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f499c3d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another way of writing the same thing is\n",
    "$$\\begin{aligned}\n",
    "    Y &= a X + b + \\epsilon \\\\\n",
    "    \\epsilon &\\sim \\text{Normal}(\\text{mean}=0, \\text{sd}=\\sigma) .\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b7e4a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now suppose we have $n$ data points:\n",
    "these will be *pairs* $(x_1, y_1), \\ldots, (x_n, y_n)$.\n",
    "We'd then like to fit the model above -- i.e., find $a$, $b$, and $\\sigma$ --\n",
    "by maximum likelihood.\n",
    "\n",
    "To do this, we need the *likelihood function*:\n",
    "$$\n",
    "    \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(y_i - a x_i - b)^2}{2 \\sigma^2}} .\n",
    "$$\n",
    "The term in the exponential is known as the *residual*,\n",
    "since it's the difference between the observed value ($y_i$)\n",
    "and the value predicted using $x$ under the model ($a x_i + b$):\n",
    "$$  y_i - a x_i - b = e_i . $$\n",
    "It is our estimate of $\\epsilon$ in the model we are trying to fit.\n",
    "\n",
    "Now, the logarithm of the likelihood function (i.e., the log-likelihood) is\n",
    "$$\n",
    "    \\ell(a, b, \\sigma)\n",
    "    =\n",
    "    -  \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n e_i^2\n",
    "    - \\frac{n}{2} \\log(2 \\pi \\sigma^2) .\n",
    "$$\n",
    "Notice that the only place that $a$ and $b$ enter in to this is via the errors, $e_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1e713",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Therefore, to find the values of $a$ and $b$ that maximize $\\ell(a, b, \\sigma)$,\n",
    "*regardless* of $\\sigma$,\n",
    "we need to *minimize* $\\sum_i e_i^2$, the sum of squared errors.\n",
    "\n",
    "In other words, the maximum likelihood estimates of $a$ and $b$\n",
    "are the *same* as the the values of $a$ and $b$ that minimize the MSE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49baaf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c6693",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now suppose that we have a bunch of variables we want to use to predict $Y$.\n",
    "Call these $X = (X_1, \\ldots, X_k)$.\n",
    "To do this, let's define coefficients $a = (a_1, \\ldots, a_k)$, and\n",
    "using them produce the estimate\n",
    "$$ \\hat Y = a_1 X_1 + \\cdots + a_k X_k , $$\n",
    "and as before find $a$ to minimize the MSE,\n",
    "$$ \\E[(Y - \\hat Y)^2] . $$\n",
    "\n",
    "Here we haven't explicitly included an intercept (before, $b$)\n",
    "because we can just assume that one of the $X_i$'s is always equal to 1\n",
    "(and if not, stick on a dummy variable that is)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4471905e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that the estimate is just the inner product of $a$ and $X$,\n",
    "i.e., $\\hat Y = a^T X$.\n",
    "Substituting this in to the MSE:\n",
    "$$\\begin{aligned}\n",
    "    \\E[(Y - \\hat Y)^2]\n",
    "    &=\n",
    "    \\E[(Y - a^T X)^2] \\\\\n",
    "    &=\n",
    "    \\E[Y^2 - 2 Y a^T X + (a^T X)^2] .\n",
    "\\end{aligned}$$\n",
    "Now, we want to find the value of $a$ that makes the derivative of this equal to zero.\n",
    "Unlike before, $a$ is a $k$-vector, not a single number,\n",
    "but multivariable calculus tells us that\n",
    "(1) $\\frac{d}{da} a^T x = x$, and (b) $\\frac{d}{da} (a^T x)^2 = 2 x x^T a$.\n",
    "So,\n",
    "$$\\begin{aligned}\n",
    "    \\frac{d}{da} \\E[(Y - \\hat Y)^2]\n",
    "    &=\n",
    "    - 2 \\E[XY] + 2 \\E[X X^T] a .\n",
    "\\end{aligned}$$\n",
    "Note that $\\E[XY]$ is a *vector*,\n",
    "whose $i^\\text{th}$ component is $\\E[X_i Y]$,\n",
    "and $\\E[X X^T]$ is a *matrix*, whose $(i,j)^\\text{th}$\n",
    "component is $\\E[X_i X_j]$.\n",
    "Think of these as the vector of covariances of $X$ with $Y$\n",
    "and the covariance matrix of $X$, respectively.\n",
    "The value of $a$ that makes this zero is\n",
    "$$\n",
    "    a = \\E[X X^T]^{-1} \\E[X Y] ,\n",
    "$$\n",
    "where $X^T$ is the transpose of $X$\n",
    "and $\\E[X X^T]^{-1}$ is the inverse of the matrix $\\E[X X^T]$, if it exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777e9fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What have we learned?\n",
    "We've found that the linear estimator of $Y$ that minimizes the mean-squared error\n",
    "can be found through some simple linear algebra\n",
    "using the covariance matrix of $X$ and the vector of correlations between $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dba86a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate linear models, take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d0190",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at that again, but from the point of view of data.\n",
    "Now suppose we have a vector of *reponses*\n",
    "$y = (y_1, \\ldots, y_n)$\n",
    "and a matrix of *predictors* $X = (x_1, \\ldots, x_n)$,\n",
    "where $x_i = (x_{i1}, \\ldots, x_{ik})$ is the $i^\\text{th}$ row of $X$.\n",
    "Think of $X$ and $y$ as *training data*,\n",
    "and we want to know how we can best predict (future) $y$ using the corresponding (future) values of $x$.\n",
    "One way to answer this is to ask:\n",
    "what are the coefficients $a = (a_1, \\ldots, a_k)$ for which\n",
    "$$ \\hat y_i = a_1 x_{i1} + \\cdots a_k x_{ik} $$\n",
    "best predicts $y_i$ in our training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb173ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mean squared error, again\n",
    "\n",
    "One way to answer this question is by finding the $a$ that minimizes\n",
    "$$ M(a) = \\sum_{i=1}^n (y_i - \\hat y_i)^2 $$\n",
    "... which, divided by $n$, is the mean squared error.\n",
    "\n",
    "Writing this out in vector notation, this is $M(a) = (y - \\hat y)^T (y - \\hat y)$,\n",
    "and since $\\hat y = X a$,\n",
    "$$\\begin{aligned}\n",
    "    M(a)\n",
    "    &=\n",
    "    y^T y - 2 a^T X^T y + a^T X^T X a .\n",
    "\\end{aligned}$$\n",
    "So,\n",
    "$$\\begin{aligned}\n",
    "    \\frac{d}{da} M(a)\n",
    "    &=\n",
    "    - 2 X^T y + 2 X^T X a ,\n",
    "\\end{aligned}$$\n",
    "and the value of $a$ that makes this zero is\n",
    "$$\n",
    "    a = (X^T X)^{-1} X^T y .\n",
    "$$\n",
    "Note the similarity to the previous theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f4e1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example\n",
    "\n",
    "(switch to the elephant example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3827c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# With scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade145ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how to do the same thing using a package, scikit-learn.\n",
    "First, we'll simulate up some simple data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c34846",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n = 200\n",
    "k = 2\n",
    "x = rng.uniform(size=(n, k))\n",
    "beta = [10, -2]\n",
    "y = x.dot(beta) + rng.normal(size=n)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(x[:,0], y); ax1.set_xlabel(f\"x0\"); ax1.set_ylabel(\"y\")\n",
    "ax2.scatter(x[:,1], y); ax2.set_xlabel(f\"x1\"); ax2.set_ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66acae4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "line_reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "line_reg.fit(x, y)\n",
    "line_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb261b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da8a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat = line_reg.predict(x)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(x[:,0], y, label='true'); ax1.set_xlabel(f\"x0\"); ax1.set_ylabel(\"y\")\n",
    "ax2.scatter(x[:,1], y); ax2.set_xlabel(f\"x1\"); ax2.set_ylabel(\"y\")\n",
    "ax1.scatter(x[:,0], y_hat, label='predicted'); ax2.scatter(x[:,1], y_hat);\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5806031",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, y_hat)\n",
    "plt.xlabel(\"observed\")\n",
    "plt.ylabel(\"predicted\");"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "rise": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
