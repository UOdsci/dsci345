{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202599bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# When there's too many right answers\n",
    "\n",
    "Peter Ralph\n",
    "\n",
    "https://uodsci.github.io/dsci345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bfdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 8)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "from dsci345 import pretty\n",
    "\n",
    "rng = np.random.default_rng(seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec5c59",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\\renewcommand{\\P}{\\mathbb{P}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\var}{\\text{var}} \\newcommand{\\sd}{\\text{sd}} \\newcommand{\\cov}{\\text{cov}} \\newcommand{\\cor}{\\text{cor}}$$\n",
    "This is here so we can use `\\P` and `\\E` and `\\var` and `\\cov` and `\\cor` and `\\sd` in LaTeX below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d439580",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = rng.uniform(high=10, size=101)\n",
    "x.sort()\n",
    "y = 10 * (1.2 * (x < np.pi) + 2 * np.logical_and(x > np.pi, x < 8) + (10 - x) * (x > 8)) - 12\n",
    "y += rng.normal(scale=2, size=len(x))\n",
    "df = pd.DataFrame({'t': x, 'y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468398c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A motivating problem: interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a639df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we've got a time series of noisy observations like the following,\n",
    "and we'd like to infer the underlying signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3784c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['t'], df['y'])\n",
    "plt.xlabel(\"time (t)\"); plt.ylabel(\"response (y)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c89c87",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Does the right answer look like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['t'], df['y'])\n",
    "plt.plot(df['t'], df['y'])\n",
    "plt.xlabel(\"time (t)\"); plt.ylabel(\"response (y)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b415127",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab97a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ey = 10 * (1.2 * (x < np.pi) + 2 * np.logical_and(x > np.pi, x < 8) + (10 - x) * (x > 8)) - 12\n",
    "plt.plot(df['t'], ey)\n",
    "plt.scatter(df['t'], df['y'])\n",
    "plt.xlabel(\"time (t)\"); plt.ylabel(\"response (y)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ad174",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da4ab81",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(df['t'], np.convolve(ey, np.ones(11)/11)[5:-5])\n",
    "plt.scatter(df['t'], df['y'])\n",
    "plt.xlabel(\"time (t)\"); plt.ylabel(\"response (y)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab509e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A quick introduction to splines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66c2ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we'd like to do is to fit a model like\n",
    "$$\n",
    "    y_i = \\beta_1 f_1(t_i) + \\cdots + \\beta_k f_k(t_i) + \\epsilon_i ,\n",
    "$$\n",
    "where $f_1(t), \\ldots, f_k(t)$ are \"nice smooth functions\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba6c40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A good way to get a bunch of \"nice smooth functions\"\n",
    "is from a \"spline basis\", like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6101cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.hstack([\n",
    "    (10/k) * patsy.dmatrix(f\"bs(t, df=k, degree=3, include_intercept=True) - 1\", df)\n",
    "    for k in (4, 8, 12, 16, 24, 32, 48, 64)\n",
    "])\n",
    " \n",
    "plt.plot(df['t'], s);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495fb931",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By adding up linear combinations of these functions\n",
    "we can make a pretty wide range of curves.\n",
    "Here's a few randomly chosen ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['t'], s.dot(rng.normal(size=(s.shape[1], 4))));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b995d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Well, we know how to fit that model!\n",
    "Least squares, here we come!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baeb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfit, _, _, _ = np.linalg.lstsq(s, y, rcond=None)\n",
    "\n",
    "plt.scatter(df['t'], df['y']);\n",
    "plt.plot(df['t'], s.dot(sfit));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8676e989",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hm - that looks a bit too... jagged? Overfit, a bit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d95d80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Too many knobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5416fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One problem is that there's too many degrees of freedom -\n",
    "we're trying to infer 208 parameters with only 101 data points.\n",
    "\n",
    "But, all those knobs make our inferred curves nice and flexible!\n",
    "Which ones do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977acb4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One solution would be to reduce things down to a smaller number of knobs\n",
    "(i.e., only use a smaller number of basis functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4313ca3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Another solution is to adjust our expectations:\n",
    "never mind, we don't actually want the **best** possible solution,\n",
    "we just want a pretty good one, please?\n",
    "And, we'd like it to be reasonable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480d47b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization, again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a32b86",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall we're fitting this model:\n",
    "$$\n",
    "    y_i = \\beta_1 f_1(t_i) + \\cdots + \\beta_k f_k(t_i) + \\epsilon_i ,\n",
    "$$\n",
    "which suggests finding $\\beta$ to minimize the loss function\n",
    "$$\n",
    "    \\sum_i \\left( y_i - \\sum_{j=1}^k \\beta_j f_j(t_i) \\right)^2 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b398a0b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To \"encourage smoothness\" we might add to this a penalty\n",
    "that depends on the wiggliness of the functions,\n",
    "say\n",
    "$$\n",
    "    + \\alpha \\sum_j \\beta_j^2 \\int f''(t)^2 dt .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475655c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We don't have the information about second derivatives easily available here,\n",
    "so I've cheated a little in how I set up the functions,\n",
    "and we can just add a \"ridge\" penalty to do roughly the same thing:\n",
    "$$ + \\alpha \\sum_j \\beta_j^2 . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629fe8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are results from using the \"ridge\" regularization,\n",
    "at different strengths.\n",
    "Which looks the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "plt.scatter(df['t'], df['y'])\n",
    "plt.plot(df['t'], s.dot(sfit), label='unpenalized')\n",
    "for a in [0.1, 2, 50, 1000]:\n",
    "    rfit = Ridge(alpha=a).fit(s, y)\n",
    "    rpred = s.dot(rfit.coef_) + rfit.intercept_\n",
    "    plt.plot(df['t'], rpred, label=f'ridge (alpha={a})');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9219e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we find a good strength of regularization?\n",
    "Crossvalidation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99797f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def do_xval(alpha, test):\n",
    "    rfit = Ridge(alpha=alpha).fit(s[~test,:], y[~test])\n",
    "    rpred = s.dot(rfit.coef_)\n",
    "    return np.sqrt(np.mean((y[test] - rpred[test])**2))\n",
    "\n",
    "def xval(alpha, folds):\n",
    "    return np.mean([do_xval(alpha, folds==j) for j in np.unique(folds)])\n",
    "\n",
    "folds = np.repeat(np.arange(10), 11)[:101]\n",
    "rng.shuffle(folds)\n",
    "\n",
    "avals = np.linspace(0.2, 2.5, 31)\n",
    "mse = np.array([xval(a, folds) for a in avals])\n",
    "a_min = avals[np.argmin(mse)]\n",
    "\n",
    "plt.plot(avals, mse);\n",
    "plt.scatter(a_min, mse[np.argmin(mse)])\n",
    "plt.xlabel(\"alpha\"); plt.ylabel(\"root mean squared testing error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226f6ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The winner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f840da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['t'], df['y'])\n",
    "plt.plot(df['t'], s.dot(sfit), label='unpenalized')\n",
    "rfit = Ridge(alpha=a_min).fit(s, y)\n",
    "rpred = s.dot(rfit.coef_) + rfit.intercept_\n",
    "plt.plot(df['t'], rpred, label=f'ridge (alpha={a_min:.4})');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce088a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What happened there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495051eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We wanted a *flexible* model, where we didn't have to pre-specify a specific, simple form for the answer.\n",
    "\n",
    "But, \"flexible\" meant there were lots of good solutions,\n",
    "and the best solutions were *too* close (suffered from overfitting).\n",
    "(The inference problem is *ill-posed*.)\n",
    "\n",
    "So, we had to be clever about how to choose a reasonable solution, out of the many good ones.\n",
    "\n",
    "This same tension between flexibility and overfitting\n",
    "is common to many methods in statistics and machine learning."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
