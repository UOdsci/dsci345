{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202599bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Robust models\n",
    "\n",
    "Peter Ralph\n",
    "\n",
    "https://uodsci.github.io/dsci345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bfdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 8)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dsci345 import pretty\n",
    "\n",
    "rng = np.random.default_rng(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec5c59",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\\renewcommand{\\P}{\\mathbb{P}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\var}{\\text{var}} \\newcommand{\\sd}{\\text{sd}} \\newcommand{\\cov}{\\text{cov}} \\newcommand{\\cor}{\\text{cor}}$$\n",
    "This is here so we can use `\\P` and `\\E` and `\\var` and `\\cov` and `\\cor` and `\\sd` in LaTeX below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f60c3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A second, *robust* linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc7dee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our first linear model\n",
    "was motivated by the probability model\n",
    "$$\\begin{aligned}\n",
    "    y_i &= x_i \\cdot \\beta + \\epsilon_i \\\\\n",
    "    \\epsilon_i &\\sim \\text{Normal}(\\text{mean}=0, \\text{sd}=\\sigma) ,\n",
    "\\end{aligned}$$\n",
    "which led us to choosing $\\beta$ by minimizing the sum of squared errors,\n",
    "$$ \\sum_i (y_i - x_i \\cdot \\beta)^2 . $$\n",
    "\n",
    "A natural question is: how much do the results depend on the \"distributional assumptions\",\n",
    "i.e., the \"Normal( )\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf2f6c7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Cauchy\n",
    "\n",
    "The Cauchy distribution is another probability distribution:\n",
    "it can take any value (like the Normal),\n",
    "its histogram looks like a hump (like the Normal),\n",
    "and it has both \"location\" and \"scale\" parameters (like the Normal).\n",
    "Let's have a look at the model\n",
    "$$\\begin{aligned}\n",
    "    y_i &= x_i \\cdot \\beta + \\epsilon_i \\\\\n",
    "    \\epsilon_i &\\sim \\text{Cauchy}(\\text{loc}=0, \\text{scale}=\\sigma) .\n",
    "\\end{aligned}$$\n",
    "\n",
    "First we'll simulate from the model\n",
    "and see how well doing the usual method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9098e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "k = 2\n",
    "x = rng.uniform(size=(n, k))\n",
    "beta = [10, -2]\n",
    "y = x.dot(beta) + rng.standard_cauchy(size=n)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(x[:,0], y); ax1.set_xlabel(f\"x0\"); ax1.set_ylabel(\"y\")\n",
    "ax2.scatter(x[:,1], y); ax2.set_xlabel(f\"x1\"); ax2.set_ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b694472",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.scatter(x[:,0], y); ax1.set_xlabel(f\"x0\"); ax1.set_ylabel(\"y\"); ax1.set_ylim((-5, 12))\n",
    "ax2.scatter(x[:,1], y); ax2.set_xlabel(f\"x1\"); ax2.set_ylabel(\"y\"); ax2.set_ylim((-5, 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5843ed8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "line_reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "line_reg.fit(x, y)\n",
    "line_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d75511",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat = line_reg.predict(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, y_hat)\n",
    "ax.set_xlim(-20, 20)\n",
    "ax.set_xlabel(\"observed value\")\n",
    "ax.set_ylabel(\"predicted value\")\n",
    "ax.set_title(f\"MSE = {np.median((y - y_hat)**2):.2f}\")\n",
    "ax.axline((0,0), slope=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f184af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What to do?\n",
    "Well, the [Cauchy density](https://en.wikipedia.org/wiki/Cauchy_distribution) with scale $\\sigma$ is\n",
    "$$\n",
    "    f(u) = \\frac{1}{\\pi \\sigma \\left(1 + (u / \\sigma)^2\\right)} .\n",
    "$$\n",
    "\n",
    "So, the likelihood of the data are\n",
    "$$ \\prod_{i=1}^n f(y_i - \\hat y_i) , $$\n",
    "and so the log-likelihood, with $\\hat y = X \\beta$,\n",
    "is \n",
    "$$    - \\sum_{i=1}^n \\log\\left( 1 + ((y_i - x_i \\beta) / \\sigma)^2 \\right) - n \\log(\\pi \\sigma) . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0b722",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def logl(u):\n",
    "    beta = u[:-1]\n",
    "    sigma = u[-1]\n",
    "    yhat = x.dot(beta)\n",
    "    return np.sum(np.log(1 + ((y - yhat) / sigma)**2)) + n * np.log(np.pi * sigma)\n",
    "\n",
    "res = minimize(logl, [1, 1, 1])\n",
    "est_beta = res['x'][:-1]\n",
    "est_sigma = res['x'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7466723",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "new_yhat = x.dot(est_beta)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y, new_yhat)\n",
    "ax.set_xlim(-20, 20)\n",
    "ax.set_xlabel(\"observed value\")\n",
    "ax.set_ylabel(\"predicted value\")\n",
    "ax.set_title(f\"MSE = {np.median((y - new_yhat)**2):.2f}\")\n",
    "ax.axline((0,0), slope=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88d55b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What did we do there?\n",
    "\n",
    "We adjusted the *loss function*:\n",
    "a more general way of describing what we're doing\n",
    "is that we're trying to minimize the *loss*\n",
    "$$ \\sum_i L(y_i - \\hat y_i), $$\n",
    "where $L( )$ is a function that quantifies how \"bad\" an \"error\" of a given size is.\n",
    "\n",
    "The standard linear model used $L(u) = u^2$;\n",
    "the Cauchy model led us to use\n",
    "$$ L(u) = \\log(1 + u^2) . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e4584",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's the difference?  Well, here's the PDFs of the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, cauchy\n",
    "fig, ax = plt.subplots()\n",
    "xvals = np.linspace(-5, 5, 101)\n",
    "ax.plot(xvals, norm.pdf(xvals), label='Normal')\n",
    "ax.plot(xvals, cauchy.pdf(xvals), label='Cauchy')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b8d53",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Those don't look *too* different.\n",
    "But have a look at the (negative) *log* PDF\n",
    "(which is what we use for a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, cauchy\n",
    "fig, ax = plt.subplots()\n",
    "xvals = np.linspace(-5, 5, 101)\n",
    "ax.plot(xvals, -1 * norm.logpdf(xvals), label='Normal')\n",
    "ax.plot(xvals, -1 * cauchy.logpdf(xvals), label='Cauchy')\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "rise": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
