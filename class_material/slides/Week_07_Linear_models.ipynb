{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202599bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to linear models\n",
    "\n",
    "Fall 2022: Peter Ralph\n",
    "\n",
    "https://uodsci.github.io/dsci345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733bfdf0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 8)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dsci345 import pretty\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec5c59",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\\renewcommand{\\P}{\\mathbb{P}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\var}{\\text{var}} \\newcommand{\\sd}{\\text{sd}} \\newcommand{\\cov}{\\text{cov}} \\newcommand{\\cor}{\\text{cor}}$$\n",
    "This is here so we can use `\\P` and `\\E` and `\\var` and `\\cov` and `\\cor` and `\\sd` in LaTeX below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f60c3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A first linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35305310",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we suppose two random variables $X$ and $Y$ depend on each other,\n",
    "perhaps the simplest set-up is that\n",
    "$$ Y = a X + b + \\epsilon, $$\n",
    "where\n",
    "- $a$ is the *slope*, i.e., how much $Y$ goes up by, on average, per unit of increase in $X$,\n",
    "- $b$ is the *intercept*, i.e., the expected value of $Y$ when $X=0$ (if this makes sense),\n",
    "- $\\hat Y = a X + b$ is the *predicted value* of $Y$ given $X$,\n",
    "- $\\epsilon = Y - \\hat Y$ is the *residual*, which for the above to be true must have mean zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7507d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e40c68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose $X$ and $Y$ are two, related, measurements,\n",
    "which we treat as random variables having some joint distribution.\n",
    "*Question:* If we know the value of $X$, how are we to best$^*$ predict the value of $Y$?\n",
    "\n",
    "$^*$ where how about \"best\" means \"with smallest mean squared error\" (i.e., smallest MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17293ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So: we'd like to define an estimator of $Y$,\n",
    "which we'll call $\\hat Y$,\n",
    "and will be a function of $X$ (and only $X$),\n",
    "and we'd like that estimator to minimize\n",
    "$$ \\E[(Y - \\hat Y)^2] \\qquad \\text{(the MSE)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32113f5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First observation\n",
    "\n",
    "If we're allowed to add a constant to our estimator,\n",
    "then the mean of the optimal estimator, $\\hat Y$, must match that of $Y$:\n",
    "$$ \\E[\\hat Y] = \\E[Y] . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9970b9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Note:* for a counterexample about the \"if we're allowed to add a constant\"\n",
    "note, suppose we're looking for an $a$ such that $\\hat Y = \\exp(a X)$;\n",
    "then $\\hat Y + m$ is *not* allowed, as it doesn't take the same form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0d9f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Proof:*\n",
    "Suppose that $\\tilde Y$ is an estimator;\n",
    "consider the MSE for $\\tilde Y + m$, where $m$ is a number,\n",
    "which we define to be $f(m)$:\n",
    "$$\\begin{aligned}\n",
    " f(m)\n",
    " &=\n",
    " \\E[(Y - (\\tilde Y + m))^2] \\\\\n",
    " &=\n",
    " \\E[(Y - \\tilde Y)^2]\n",
    " - 2 m \\E[Y - \\tilde Y]\n",
    " + m^2 .\n",
    "\\end{aligned}$$\n",
    "We want to find the value of $m$ that minimizes $f(m)$,\n",
    "so differentiate with respect to $m$:\n",
    "$$\\begin{aligned}\n",
    " f'(m)\n",
    " &=\n",
    " - 2\\E[Y - \\tilde Y]\n",
    " + 2 m .\n",
    "\\end{aligned}$$\n",
    "Setting this equal to zero, we find that $f'(m) = 0$ if\n",
    "$$ m = \\E[Y - \\tilde Y] = \\E[Y] - \\E[\\tilde Y]. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c863d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In conclusion, we've found out that if $\\tilde Y$ is an estimator,\n",
    "then $\\hat Y = \\tilde Y + \\E[Y] - \\E[\\tilde Y]$ is a *better* estimator.\n",
    "And, notice that (by additivity of means),\n",
    "$\\E[\\hat Y] = \\E[Y]$.\n",
    "\n",
    "So, any estimator of $Y$ can be improved (in the mean-squared-error sense)\n",
    "by shifting it so that the mean of the estimator is equal to the mean of $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35c87f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Second observation\n",
    "\n",
    "If the estimator $\\hat Y$ is *linear*,\n",
    "i.e.,\n",
    "$$ \\hat Y = a X + b $$\n",
    "for some $a$ and $b$, then\n",
    "$$\n",
    "a = \\frac{\\sd[Y]}{\\sd[X]} \\cor[X, Y] ,\n",
    "$$\n",
    "and $b$ is chosen so that $\\E[\\hat Y] = \\E[Y]$:\n",
    "$$ b = \\E[Y] - a \\E[X] .$$\n",
    "\n",
    "In words, the slope, $a$,\n",
    "is equal to the correlation between $X$ and $Y$,\n",
    "but in units of standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6e068",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Proof:*\n",
    "We'll do just the same thing as above.\n",
    "First, thanks to the first observation,\n",
    "we can assume $\\E[X] = \\E[Y] = 0$,\n",
    "which implies that $b = 0$,\n",
    "and $\\E[X^2] = \\var[X]$ and $\\E[Y^2] = \\var[Y]$\n",
    "and $\\E[XY] = \\cov[X,Y]$.\n",
    "Now,\n",
    "$$\\begin{aligned}\n",
    "    \\text{(MSE)}\n",
    "    &= \\E[(Y - \\hat Y)^2 ] \\\\\n",
    "    &= \\E[(Y - a X)^2 ] \\\\\n",
    "    &= \\E[Y^2 - 2 a X Y + a^2 X^2 ] \\\\\n",
    "    &= \\var[Y] - 2 a \\cov[X, Y] + a^2 \\var[X] .\n",
    "\\end{aligned}$$\n",
    "If we differentiate this with respect to $a$\n",
    "and set it equal to zero,\n",
    "we find that the MSE is minimized at\n",
    "$$ a = \\frac{\\cov[X,Y]}{\\var[X]} . $$\n",
    "If we now substitute in for $\\cov[X,Y] = \\sd[X]\\sd[Y]\\cor[X,Y]$,\n",
    "we get the form of $a$ above."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "rise": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
