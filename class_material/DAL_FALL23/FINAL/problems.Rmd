---
title: "Final Solutions"
output:
  pdf_document: default
  html_document: default
date: "2023-11-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
library(xtable)
```

## Problem 1

```{r, shuttle, echo=FALSE, fig.show='hide', fig.path = "FIGURES/"}
shuttle <- read.csv("http://pages.uoregon.edu/dlevin/DATA/shuttle.csv",header=TRUE)
plot(I(fail+runif(24,-0.1,0.025))~temp, data=shuttle, pch=19, xlim=c(30,85),  ylim=c(-0.1,1.1),ylab="prob of failure", cex=0.8)
f <- glm(fail~temp, family=binomial, data=shuttle)
t0 <- seq(30,85,0.1)
p0 <- predict(f, newdata=data.frame(temp=t0),type = "response")
lines(t0,p0)
pred.data <- c(50,75,31)
p.pred <- predict(f, newdata=data.frame(temp=pred.data), type="response")
```

The data along with the fitted logistic regression model is shown in Figure \ref{Fig:All}.

![Data with fitted model](FIGURES/shuttle-1){#Fig:All}

The estimated failure probabilities are given in Table \ref{Tab:EFP}.

```{r, echo=FALSE, results='asis'}
pr.tble <- data.frame(temp=pred.data,prob=p.pred)
xtable(pr.tble,caption="Estimated failure probabilities",label="Tab:EFP",digits=3)
```

Since the data used to fit the model all lies above 50 degrees, the prediction at 31 degrees is extrapolated beyond where we can have confidence in the model.  Thus the probability of failure near one at 31 degrees is subject to model uncertainty.  Nonetheless, the estimated failure probability at 50 degrees is already estimated above 90\%, so we conclude that failure at 31 degrees is with reasonable confidence quite high indeed.

## Problem 4

```{r, eval=FALSE, echo=FALSE}
set.seed(7)
x <- runif(15,-3,3)
y <- x^4/4  +x -2*x^2 +rnorm(15,0,0.5)
xy <- data.frame(x,y)
write.table(xy,file = "~/Desktop/xy.txt",row.names=FALSE)
```


```{r, poly, echo=FALSE, fig.show='hide', fig.path="FIGURES/"}
xy <- read.table("http://pages.uoregon.edu/dlevin/DATA/xy2023.txt",header=TRUE)
plot(y~x, data=xy, ylim=c(-6,6))
x0 <- seq(-4,4,0.1)
for(i in 1:10){
  f <- lm(y~poly(x,deg=i), data=xy)
  y0 <- predict(f, newdata=data.frame(x=x0))
  lines(x0,y0,col=i)
}
#y1 <- x0^4/4 +x0 - 2*x0^2
#lines(x0,y1,lwd=3)
```

The data, along with the best-fitting polynomials up to degree $10$ are shown in Figure \ref{Fig:PFits}.

![Data with fitted model](FIGURES/poly-1){#Fig:PFits}

```{r, echo=FALSE}
kpart <- function(df,k){
  n = dim(df)[1]
  ri <- sample(1:n)
  sp <- c(0,round(quantile(1:n,(1:(k-1))/k)),n)
  
  kfolds <- vector(mode='list', length=k)
  for(i in 1:k){
    ind_test <- ri[(sp[i]+1):sp[(i+1)]]
    ind_train <- (1:n)[-ind_test]
    test_df <- df[ind_test,]
    train_df <- df[ind_train,]
    kfolds[[i]] <- list(train=train_df,test=test_df)
  }
  return(kfolds)
}
```

```{r, rmsep, echo=FALSE, fig.show='hide', fig.path="FIGURES/"}
xyk <- kpart(xy,5)

rmsepd <- 1:10
for(d in 1:10){
rmsep <- 1:5
for(i in 1:5){
  f1 <- lm(y~poly(x,d), data=xyk[[i]]$train)
  yf <- predict(f1, newdata=xyk[[i]]$test)
  rmsep[i] <- sqrt(mean((xyk[[i]]$test$y-yf)^2))
  }
 rmsepd[d] <- mean(rmsep) 
}
plot(1:7,rmsepd[1:7], pch=19)
```

The estimated RMSEP is shown against polynomial degree in Figure \ref{Fig:RMSEP}.  The RMSEP was estimated using $5$-fold cross-validation.  The RMSEP is minimized at degree $4$, thus this model is likely to perform best when predicting new data.


![Estimated RMSEP vs polynomial degree](FIGURES/rmsep-1){#Fig:RMSEP}
